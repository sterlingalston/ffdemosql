-- =============================================================================
-- MERGE TABLES FROM db_ff_external_sql TO db_ff_demo WITH DYNAMIC ROW_HASH
-- Calculates ROW_HASH on the fly without adding columns to source database
-- 
-- MERGE Logic:
-- - INSERT (new primary key): Sets DATETIME_CREATED and DATETIME_UPDATED to current timestamp
-- - UPDATE (existing PK, different ROW_HASH): Sets DATETIME_UPDATED to current timestamp
-- - NO ACTION (existing PK, same ROW_HASH): No changes made
-- =============================================================================

CREATE OR REPLACE PROCEDURE MERGE_EXTERNAL_TO_DEMO(
    SOURCE_DATABASE STRING DEFAULT 'DB_FF_EXTERNAL_SQL',
    TARGET_DATABASE STRING DEFAULT 'DB_FF_DEMO',
    EXCLUDE_COLUMNS STRING DEFAULT 'ROW_HASH,DATETIME_UPDATED,DATETIME_CREATED,CREATED_DATE,CREATED_AT,LAST_UPDATED,MODIFIED_DATE',
    SCHEMA_FILTER STRING DEFAULT '%',
    TABLE_FILTER STRING DEFAULT '%',
    DRY_RUN BOOLEAN DEFAULT FALSE
)
RETURNS VARIANT
LANGUAGE PYTHON
RUNTIME_VERSION = '3.9'
PACKAGES = ('snowflake-snowpark-python')
HANDLER = 'main'
AS
$$
def main(session, source_database, target_database, exclude_columns, schema_filter, table_filter, dry_run):
    """
    Merge tables from db_ff_external_sql to db_ff_demo with dynamic ROW_HASH calculation.
    """
    try:
        results = {
            'source_database': source_database,
            'target_database': target_database,
            'tables_processed': 0,
            'tables_skipped': 0,
            'total_rows_merged': 0,
            'merge_results': [],
            'errors': [],
            'dry_run': dry_run
        }
        
        # Parse excluded columns
        excluded_cols = [col.strip().upper() for col in exclude_columns.split(',')]
        
        # Get matching tables from both databases
        matching_tables = get_matching_tables(session, source_database, target_database, schema_filter, table_filter)
        
        for table_info in matching_tables:
            schema_name = table_info['schema']
            table_name = table_info['table']
            
            try:
                # Process each table
                table_result = merge_single_table(
                    session,
                    source_database,
                    target_database,
                    schema_name,
                    table_name,
                    excluded_cols,
                    dry_run
                )
                
                if table_result['success']:
                    results['tables_processed'] += 1
                    results['total_rows_merged'] += table_result.get('rows_affected', 0)
                    results['merge_results'].append(table_result)
                else:
                    results['tables_skipped'] += 1
                    results['errors'].append({
                        'table': f"{schema_name}.{table_name}",
                        'error': table_result.get('error', 'Unknown error')
                    })
                    
            except Exception as table_error:
                results['tables_skipped'] += 1
                results['errors'].append({
                    'table': f"{schema_name}.{table_name}",
                    'error': str(table_error)
                })
        
        return results
        
    except Exception as e:
        return {
            'error': f"Fatal error: {str(e)}",
            'source_database': source_database,
            'target_database': target_database
        }

def get_matching_tables(session, source_db, target_db, schema_filter, table_filter):
    """
    Find tables that exist in both source and target databases.
    """
    # Get tables from source database - using fully qualified names
    source_query = f"""
    SELECT TABLE_SCHEMA, TABLE_NAME
    FROM {source_db}.INFORMATION_SCHEMA.TABLES 
    WHERE TABLE_TYPE = 'BASE TABLE'
      AND TABLE_SCHEMA != 'INFORMATION_SCHEMA'
      AND TABLE_SCHEMA ILIKE '{schema_filter}'
      AND TABLE_NAME ILIKE '{table_filter}'
    """
    
    # Get tables from target database that have ROW_HASH column - using fully qualified names
    target_query = f"""
    SELECT DISTINCT t.TABLE_SCHEMA, t.TABLE_NAME
    FROM {target_db}.INFORMATION_SCHEMA.TABLES t
    INNER JOIN {target_db}.INFORMATION_SCHEMA.COLUMNS c
        ON t.TABLE_SCHEMA = c.TABLE_SCHEMA 
        AND t.TABLE_NAME = c.TABLE_NAME
    WHERE t.TABLE_TYPE = 'BASE TABLE'
      AND c.COLUMN_NAME = 'ROW_HASH'
      AND t.TABLE_SCHEMA ILIKE '{schema_filter}'
      AND t.TABLE_NAME ILIKE '{table_filter}'
    """
    
    # Execute queries with fully qualified names - no USE statements needed
    source_tables_df = session.sql(source_query)
    source_tables = [(row['TABLE_SCHEMA'], row['TABLE_NAME']) for row in source_tables_df.collect()]
    
    target_tables_df = session.sql(target_query)
    target_tables = [(row['TABLE_SCHEMA'], row['TABLE_NAME']) for row in target_tables_df.collect()]
    
    # Find intersection
    matching_tables = []
    for schema, table in source_tables:
        if (schema, table) in target_tables:
            matching_tables.append({'schema': schema, 'table': table})
    
    return matching_tables

def get_table_structure(session, database, schema, table, excluded_cols):
    """
    Analyze table structure and identify primary keys and hash columns.
    """
    # Get column information using fully qualified database name
    columns_query = f"""
    SELECT 
        COLUMN_NAME,
        DATA_TYPE,
        IS_NULLABLE,
        IS_IDENTITY,
        ORDINAL_POSITION
    FROM {database}.INFORMATION_SCHEMA.COLUMNS 
    WHERE TABLE_SCHEMA = '{schema}' 
      AND TABLE_NAME = '{table}'
    ORDER BY ORDINAL_POSITION
    """
    
    columns_df = session.sql(columns_query)
    columns_list = columns_df.collect()
    
    # Identify primary key candidates and hash columns
    primary_key_columns = []
    hash_columns = []
    all_columns = []
    
    for col_row in columns_list:
        col_name = col_row['COLUMN_NAME']
        col_upper = col_name.upper()
        data_type = col_row['DATA_TYPE']
        is_identity = col_row['IS_IDENTITY']
        
        all_columns.append({
            'name': col_name,
            'data_type': data_type,
            'is_identity': is_identity
        })
        
        # Skip excluded columns
        if col_upper in excluded_cols:
            continue
            
        # Identify primary key candidates
        if (is_identity == 'YES' or 
            col_upper.endswith('_ID') or 
            col_upper == 'ID' or
            col_upper.endswith('ID')):
            primary_key_columns.append(col_name)
        else:
            hash_columns.append({
                'name': col_name,
                'data_type': data_type
            })
    
    # If no identity columns found, use first ID-like column as primary key
    if not primary_key_columns:
        for col in all_columns:
            col_upper = col['name'].upper()
            if (col_upper.endswith('_ID') or col_upper == 'ID'):
                primary_key_columns.append(col['name'])
                # Remove from hash columns if it was there
                hash_columns = [hc for hc in hash_columns if hc['name'] != col['name']]
                break
    
    return {
        'primary_keys': primary_key_columns,
        'hash_columns': hash_columns,
        'all_columns': all_columns
    }

def build_hash_expression(hash_columns):
    """
    Build MD5 hash expression for given columns.
    """
    if not hash_columns:
        return "'NO_HASHABLE_COLUMNS'"
    
    hash_parts = []
    for col_info in hash_columns:
        col_name = col_info['name']
        data_type = col_info['data_type'].upper()
        
        # Handle different data types for consistent hashing
        if any(t in data_type for t in ['VARCHAR', 'STRING', 'TEXT', 'CHAR']):
            hash_parts.append(f"COALESCE(TRIM({col_name}), '')")
        elif any(t in data_type for t in ['NUMBER', 'INT', 'DECIMAL', 'NUMERIC']):
            hash_parts.append(f"COALESCE(TO_VARCHAR({col_name}), '')")
        elif any(t in data_type for t in ['FLOAT', 'DOUBLE', 'REAL']):
            hash_parts.append(f"COALESCE(TO_VARCHAR(ROUND({col_name}, 10)), '')")
        elif any(t in data_type for t in ['DATE', 'TIME']):
            hash_parts.append(f"COALESCE(TO_VARCHAR({col_name}), '')")
        elif 'BOOLEAN' in data_type:
            hash_parts.append(f"COALESCE(TO_VARCHAR({col_name}), '')")
        elif any(t in data_type for t in ['VARIANT', 'OBJECT', 'ARRAY']):
            hash_parts.append(f"COALESCE(TO_VARCHAR({col_name}), '')")
        elif 'BINARY' in data_type:
            hash_parts.append(f"COALESCE(HEX_ENCODE({col_name}), '')")
        else:
            hash_parts.append(f"COALESCE(TO_VARCHAR({col_name}), '')")
    
    concat_expr = " || '|' || ".join(hash_parts)
    return f"MD5({concat_expr})"

def merge_single_table(session, source_db, target_db, schema, table, excluded_cols, dry_run):
    """
    Merge a single table from source to target database.
    """
    try:
        # Get table structure from source
        source_structure = get_table_structure(session, source_db, schema, table, excluded_cols)
        
        # Get table structure from target (to verify it has ROW_HASH)
        target_structure = get_table_structure(session, target_db, schema, table, [])
        
        if not source_structure['primary_keys']:
            return {
                'success': False,
                'error': 'No primary key columns identified',
                'table': f"{schema}.{table}"
            }
        
        # Verify target has ROW_HASH column
        target_has_row_hash = any(col['name'].upper() == 'ROW_HASH' for col in target_structure['all_columns'])
        if not target_has_row_hash:
            return {
                'success': False,
                'error': 'Target table does not have ROW_HASH column',
                'table': f"{schema}.{table}"
            }
        
        # Build hash expression for source data
        hash_expression = build_hash_expression(source_structure['hash_columns'])
        
        # Build column lists for MERGE
        source_columns = [col['name'] for col in source_structure['all_columns']]
        primary_keys = source_structure['primary_keys']
        
        # Build MERGE statement
        merge_sql = build_merge_statement(
            source_db, target_db, schema, table,
            source_columns, primary_keys, hash_expression
        )
        
        if dry_run:
            return {
                'success': True,
                'table': f"{schema}.{table}",
                'primary_keys': primary_keys,
                'hash_columns': [col['name'] for col in source_structure['hash_columns']],
                'merge_sql': merge_sql[:500] + '...' if len(merge_sql) > 500 else merge_sql,
                'rows_affected': 0
            }
        
        # Execute the MERGE - no USE statement needed, fully qualified target table
        result = session.sql(merge_sql).collect()
        
        # Try to get row count (this might not always work)
        try:
            row_count_query = "SELECT SQLROWCOUNT as ROWS_AFFECTED"
            row_count_result = session.sql(row_count_query).collect()
            rows_affected = row_count_result[0]['ROWS_AFFECTED'] if row_count_result else 0
        except:
            rows_affected = 0
        
        return {
            'success': True,
            'table': f"{schema}.{table}",
            'primary_keys': primary_keys,
            'hash_columns': [col['name'] for col in source_structure['hash_columns']],
            'rows_affected': rows_affected,
            'merge_executed': True
        }
        
    except Exception as e:
        return {
            'success': False,
            'error': str(e),
            'table': f"{schema}.{table}"
        }

def build_merge_statement(source_db, target_db, schema, table, source_columns, primary_keys, hash_expression):
    """
    Build the dynamic MERGE statement.
    """
    full_source_table = f"{source_db}.{schema}.{table}"
    full_target_table = f"{target_db}.{schema}.{table}"
    
    # Build join condition on primary keys
    join_conditions = [f"target.{pk} = source.{pk}" for pk in primary_keys]
    join_condition = " AND ".join(join_conditions)
    
    # Build column lists (exclude ROW_HASH, DATETIME_UPDATED, and DATETIME_CREATED from source)
    insert_columns = [col for col in source_columns if col.upper() not in ['ROW_HASH', 'DATETIME_UPDATED', 'DATETIME_CREATED']]
    update_columns = [col for col in source_columns if col.upper() not in ['ROW_HASH', 'DATETIME_UPDATED', 'DATETIME_CREATED'] and col not in primary_keys]
    
    # Build UPDATE SET clause - only update DATETIME_UPDATED when ROW_HASH changes
    update_sets = [f"{col} = source.{col}" for col in update_columns]
    update_sets.append(f"ROW_HASH = source.calculated_hash")
    update_sets.append("DATETIME_UPDATED = CURRENT_TIMESTAMP()")
    update_set_clause = ",\n        ".join(update_sets)
    
    # Build INSERT columns and values - set both DATETIME_CREATED and DATETIME_UPDATED on insert
    insert_cols_with_tracking = insert_columns + ['ROW_HASH', 'DATETIME_CREATED', 'DATETIME_UPDATED']
    insert_values = [f"source.{col}" for col in insert_columns]
    insert_values.extend(['source.calculated_hash', 'CURRENT_TIMESTAMP()', 'CURRENT_TIMESTAMP()'])
    
    insert_columns_clause = ", ".join(insert_cols_with_tracking)
    insert_values_clause = ", ".join(insert_values)
    
    # Build the complete MERGE statement with fully qualified table names
    merge_sql = f"""
MERGE INTO {full_target_table} target
USING (
    SELECT 
        *,
        {hash_expression} as calculated_hash
    FROM {full_source_table}
) source 
ON {join_condition}
WHEN MATCHED AND target.ROW_HASH != source.calculated_hash THEN 
    -- Update existing record when data has changed
    UPDATE SET 
        {update_set_clause}
WHEN NOT MATCHED THEN 
    -- Insert new record with creation and update timestamps
    INSERT ({insert_columns_clause})
    VALUES ({insert_values_clause})
"""
    
    return merge_sql
$$;

-- =============================================================================
-- SIMPLIFIED VERSION FOR SINGLE TABLE MERGE
-- =============================================================================

CREATE OR REPLACE PROCEDURE MERGE_SINGLE_TABLE_EXTERNAL_TO_DEMO(
    SCHEMA_NAME STRING,
    TABLE_NAME STRING,
    PRIMARY_KEY_COLUMNS STRING, -- Comma-separated list of PK columns
    SOURCE_DATABASE STRING DEFAULT 'DB_FF_EXTERNAL_SQL',
    TARGET_DATABASE STRING DEFAULT 'DB_FF_DEMO',
    DRY_RUN BOOLEAN DEFAULT FALSE
)
RETURNS VARIANT
LANGUAGE PYTHON
RUNTIME_VERSION = '3.9'
PACKAGES = ('snowflake-snowpark-python')
HANDLER = 'main'
AS
$$
def main(session, schema_name, table_name, primary_key_columns, source_database, target_database, dry_run):
    """
    Merge a single table from external SQL to demo database.
    """
    try:
        # Parse primary key columns
        pk_columns = [col.strip() for col in primary_key_columns.split(',')]
        
        # Get source table structure using fully qualified name
        columns_query = f"""
        SELECT COLUMN_NAME, DATA_TYPE
        FROM {source_database}.INFORMATION_SCHEMA.COLUMNS 
        WHERE TABLE_SCHEMA = '{schema_name}' 
          AND TABLE_NAME = '{table_name}'
        ORDER BY ORDINAL_POSITION
        """
        
        columns_df = session.sql(columns_query)
        columns_list = columns_df.collect()
        
        # Identify hash columns (exclude PKs and tracking columns)
        excluded_cols = ['ROW_HASH', 'DATETIME_UPDATED', 'DATETIME_CREATED', 'CREATED_DATE', 'CREATED_AT', 'LAST_UPDATED']
        hash_columns = []
        all_columns = []
        
        for col_row in columns_list:
            col_name = col_row['COLUMN_NAME']
            data_type = col_row['DATA_TYPE']
            all_columns.append(col_name)
            
            if (col_name not in pk_columns and 
                col_name.upper() not in excluded_cols):
                hash_columns.append({
                    'name': col_name,
                    'data_type': data_type
                })
        
        # Build hash expression
        if not hash_columns:
            hash_expr = "'NO_HASHABLE_COLUMNS'"
        else:
            hash_parts = []
            for col_info in hash_columns:
                col_name = col_info['name']
                data_type = col_info['data_type'].upper()
                
                if any(t in data_type for t in ['VARCHAR', 'STRING', 'TEXT', 'CHAR']):
                    hash_parts.append(f"COALESCE(TRIM({col_name}), '')")
                else:
                    hash_parts.append(f"COALESCE(TO_VARCHAR({col_name}), '')")
            
            concat_expr = " || '|' || ".join(hash_parts)
            hash_expr = f"MD5({concat_expr})"
        
        # Build MERGE statement with fully qualified table names
        source_table = f"{source_database}.{schema_name}.{table_name}"
        target_table = f"{target_database}.{schema_name}.{table_name}"
        
        # Join condition
        join_conditions = [f"target.{pk} = source.{pk}" for pk in pk_columns]
        join_condition = " AND ".join(join_conditions)
        
        # Update columns (exclude PKs and tracking columns)
        update_columns = [col for col in all_columns if col not in pk_columns and col.upper() not in ['ROW_HASH', 'DATETIME_UPDATED', 'DATETIME_CREATED']]
        update_sets = [f"{col} = source.{col}" for col in update_columns]
        update_sets.extend(['ROW_HASH = source.calculated_hash', 'DATETIME_UPDATED = CURRENT_TIMESTAMP()'])
        
        # Insert columns (exclude tracking columns, add them manually)
        insert_columns = [col for col in all_columns if col.upper() not in ['ROW_HASH', 'DATETIME_UPDATED', 'DATETIME_CREATED']]
        insert_columns.extend(['ROW_HASH', 'DATETIME_CREATED', 'DATETIME_UPDATED'])
        insert_values = [f"source.{col}" for col in all_columns if col.upper() not in ['ROW_HASH', 'DATETIME_UPDATED', 'DATETIME_CREATED']]
        insert_values.extend(['source.calculated_hash', 'CURRENT_TIMESTAMP()', 'CURRENT_TIMESTAMP()'])
        
        merge_sql = f"""
MERGE INTO {target_table} target
USING (
    SELECT 
        *,
        {hash_expr} as calculated_hash
    FROM {source_table}
) source 
ON {join_condition}
WHEN MATCHED AND target.ROW_HASH != source.calculated_hash THEN 
    -- Update existing record when data has changed
    UPDATE SET 
        {', '.join(update_sets)}
WHEN NOT MATCHED THEN 
    -- Insert new record with creation and update timestamps
    INSERT ({', '.join(insert_columns)})
    VALUES ({', '.join(insert_values)})
"""
        
        if dry_run:
            return {
                'table': f"{schema_name}.{table_name}",
                'primary_keys': pk_columns,
                'hash_columns': [col['name'] for col in hash_columns],
                'merge_sql': merge_sql,
                'dry_run': True
            }
        
        # Execute the merge - no USE statement needed
        session.sql(merge_sql).collect()
        
        return {
            'table': f"{schema_name}.{table_name}",
            'primary_keys': pk_columns,
            'hash_columns': [col['name'] for col in hash_columns],
            'success': True,
            'merge_executed': True
        }
        
    except Exception as e:
        return {
            'error': str(e),
            'table': f"{schema_name}.{table_name}"
        }
$$;

-- =============================================================================
-- USAGE EXAMPLES
-- =============================================================================

-- Option 1: Merge all matching tables (dry run first)
CALL MERGE_EXTERNAL_TO_DEMO('DB_FF_EXTERNAL_SQL', 'DB_FF_DEMO', 'ROW_HASH,DATETIME_UPDATED,DATETIME_CREATED,CREATED_DATE', '%', '%', TRUE);

-- Execute for real
CALL MERGE_EXTERNAL_TO_DEMO('DB_FF_EXTERNAL_SQL', 'DB_FF_DEMO', 'ROW_HASH,DATETIME_UPDATED,DATETIME_CREATED,CREATED_DATE', '%', '%', FALSE);

-- Option 2: Merge specific schema
CALL MERGE_EXTERNAL_TO_DEMO('DB_FF_EXTERNAL_SQL', 'DB_FF_DEMO', 'ROW_HASH,DATETIME_UPDATED,DATETIME_CREATED,CREATED_DATE', 'CORE', '%', FALSE);

-- Option 3: Merge single table
CALL MERGE_SINGLE_TABLE_EXTERNAL_TO_DEMO('CORE', 'CUSTOMERS', 'CUSTOMER_ID', 'DB_FF_EXTERNAL_SQL', 'DB_FF_DEMO', TRUE);
CALL MERGE_SINGLE_TABLE_EXTERNAL_TO_DEMO('PAYMENTS', 'TRANSACTIONS', 'TRANSACTION_ID', 'DB_FF_EXTERNAL_SQL', 'DB_FF_DEMO', FALSE);

-- =============================================================================
-- VERIFICATION QUERIES
-- =============================================================================

-- Check merge results
SELECT 
    'CUSTOMERS' as TABLE_NAME,
    COUNT(*) as TOTAL_ROWS,
    COUNT(CASE WHEN ROW_HASH IS NOT NULL THEN 1 END) as ROWS_WITH_HASH,
    MAX(DATETIME_UPDATED) as LAST_UPDATED
FROM DB_FF_DEMO.CORE.CUSTOMERS

UNION ALL

SELECT 
    'TRANSACTIONS' as TABLE_NAME,
    COUNT(*) as TOTAL_ROWS,
    COUNT(CASE WHEN ROW_HASH IS NOT NULL THEN 1 END) as ROWS_WITH_HASH,
    MAX(DATETIME_UPDATED) as LAST_UPDATED
FROM DB_FF_DEMO.PAYMENTS.TRANSACTIONS;

-- Check recently updated records
SELECT 
    CUSTOMER_ID,
    FIRST_NAME,
    LAST_NAME,
    LEFT(ROW_HASH, 16) || '...' as HASH_PREVIEW,
    DATETIME_UPDATED
FROM DB_FF_DEMO.CORE.CUSTOMERS 
WHERE DATETIME_UPDATED >= CURRENT_TIMESTAMP() - INTERVAL '1 HOUR'
ORDER BY DATETIME_UPDATED DESC
LIMIT 10;

select * from core.accounts